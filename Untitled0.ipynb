{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTBOTz9rR5oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r59zZoNS6-Xi",
        "colab_type": "code",
        "outputId": "d3e0183f-a37e-4292-f780-7d3085c231db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K     |████████████████████████████████| 592.3MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.16.3)\n",
            "\u001b[31mERROR: fastai 1.0.52 has requirement torch>=1.0.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.1.0\n",
            "    Uninstalling torch-1.1.0:\n",
            "      Successfully uninstalled torch-1.1.0\n",
            "Successfully installed torch-0.3.0.post4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoFP2MN8nwgr",
        "colab_type": "code",
        "outputId": "d42a4907-bc92-46e6-de53-4eba5cb1597c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "if torch.cuda.is_available():\n",
        "  VOCAB_SIZE = 30000\n",
        "  print('Using Cuda')\n",
        "else:\n",
        "  VOCAB_SIZE = 5000\n",
        "\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "WINDOW_SIZE = 5\n",
        "BATCH_SIZE = 1024\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1ENX-gsfx4",
        "colab_type": "code",
        "outputId": "a3a9a99b-f745-4ccc-e787-fd21d4ac1895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "words = []\n",
        "with open(\"text8.txt\") as f:\n",
        "  for line in f.readlines():\n",
        "    words += line.strip().split(\" \") #라인을 하나씩 읽어들이며 띄어쓰기 단위로 단어들을 분리하여 words array에 집어넣음\n",
        "    \n",
        "print(\"total words in corpus: %d\" % (len(words))) #총 words array 안에 들어간 단어 개수\n",
        "\n",
        "word_cnt = Counter() #컨테이너에 동일한 값의 자료가 몇개인지를 파악하는데 사용하는 객체\n",
        "\n",
        "for w in words:\n",
        "  if w not in word_cnt: #한번도 나타나지 않았던 단어라면 0으로 셋팅\n",
        "    word_cnt[w] = 0\n",
        "  word_cnt[w] += 1 #나타난 단어라면 카운트하기\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total words in corpus: 7126540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBoSRWHwskCB",
        "colab_type": "code",
        "outputId": "e2cc569d-068a-4117-984f-9a496c7e0f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# calculate word coverage of 30k most common words\n",
        "total = 0\n",
        "\n",
        "# print(word_cnt.most_common(VOCAB_SIZE)[0]) ('the', 1061396)\n",
        "\n",
        "for cnt_tup in word_cnt.most_common(VOCAB_SIZE): #word_cnt에서 most common word 3만개 추출\n",
        "  total += cnt_tup[1] \n",
        "print(\"coverage: %.4f \" % (total * 1.0 / len(words))) #전체 단어대비 추출된 단어의 커버리지 퍼센트\n",
        "# 95.94%\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "coverage: 0.9629 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN6ysgvJ4v8e",
        "colab_type": "code",
        "outputId": "93816c9a-787d-437f-8445-ba52b24f8667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# make vocabulary with most common words\n",
        "word_to_ix = dict()\n",
        "\n",
        "for i, cnt_tup in enumerate(word_cnt.most_common(VOCAB_SIZE)):\n",
        "  word_to_ix[cnt_tup[0]] = i # 3만개의 각 단어에 대해 고유숫자를 부여\n",
        "  last = i\n",
        "  \n",
        "print(word_to_ix['banana'])\n",
        "print(len(word_to_ix))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11447\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDBGby4g5JRz",
        "colab_type": "code",
        "outputId": "36881cf9-b28d-439b-c802-930ffa803a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# add unk token to vocabulary\n",
        "word_to_ix[UNK_TOKEN] = len(word_to_ix) #UNK_TOKEN에 가장 마지막 고유숫자를 부여\n",
        "\n",
        "# replace rare words in train data with UNK_TOKEN\n",
        "train_words = []\n",
        "for w in words:\n",
        "  if w not in word_to_ix: #전체단어 중 word_to_ix(고유숫자가 부여된 3만개의 단어)에 없는 것이라면 UNK로 넣기\n",
        "    train_words += [UNK_TOKEN]\n",
        "  else:\n",
        "    train_words += [w] #있는 것이라면 해당 단어를 그대로 넣기\n",
        "    \n",
        "print(len(train_words))\n",
        "print(train_words[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7126540\n",
            "anarchism\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d_FxQym5Wn8",
        "colab_type": "code",
        "outputId": "19cac138-5d62-4573-892f-67d8812e9486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# make train samples for CBOW\n",
        "train_input = []  #window size 만큼의 주변 context words\n",
        "train_target = [] #context words를 통해 맞춰야 하는 center word\n",
        "span = (WINDOW_SIZE - 1) // 2 #span = 2\n",
        "for i in range(span, len(train_words) - span): # center word 등록하는 숫자\n",
        "  context = []\n",
        "  for j in range(WINDOW_SIZE): # center word 당 주변 context words 등록\n",
        "    if j != span:\n",
        "      context.append(word_to_ix[train_words[i + j - span]]) # center word에서 span뺀만큼에서 i더한 위치\n",
        "  target = word_to_ix[train_words[i]]\n",
        "  train_input.append(context)\n",
        "  train_target.append(target)\n",
        "print(\"data is generated!\")\n",
        "print(train_input[0],train_target[0])\n",
        "\n",
        "for k, v in word_to_ix.items(): #word_to_ix에 아이템을 하나씩 접근해서, key, value를 각각 name, age에 저장\n",
        "    if v == train_target[0]:\n",
        "        print(k)\n",
        "        \n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data is generated!\n",
            "[3970, 2941, 5, 181] 11\n",
            "as\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ0YF4vI5gj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model class\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, window_size):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim) #input layer to projection layer\n",
        "    self.linear1 = nn.Linear(embedding_dim, vocab_size) #projection layer to output layer\n",
        "    pass\n",
        "\n",
        "  def forward(self, inputs): #stacking each layer together\n",
        "    embeds = self.embeddings(inputs)\n",
        "    out = self.linear1(torch.mean(embeds, dim=1)) #임베딩된 결과를 평균함\n",
        "    log_probs = F.log_softmax(out, dim=1) #임베딩된 결과에 활성함수를 취함\n",
        "    return log_probs\n",
        "\n",
        "  def get_word_emdedding(self, word):\n",
        "    word = torch.cuda.LongTensor([word_to_ix[word]]) #int를 torch에서 사용하는 자료형으로 변환(?)\n",
        "    return self.embeddings(word).view(1, -1) # ?????\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t5_M7koLNA01",
        "colab": {}
      },
      "source": [
        "# set up to train\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss() #음의 로그 우도 손실 / NLLLoss에 대한 입력 은 로그 확률의 벡터이고 목표 레이블입니다.\n",
        "model = CBOW(len(word_to_ix), EMBEDDING_DIM, WINDOW_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYwTfZsf57TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if gpu is available, then use it\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH_8g9KRP9bD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90ba7608-8928-4d28-8045-d60604cf6246"
      },
      "source": [
        "# make data loader for batch training\n",
        "# Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "# into integer indices and wrap them in tensors)\n",
        "train_input = torch.from_numpy(np.asarray(train_input)).long() # Creates a Tensor from a numpy.ndarray\n",
        "train_target = torch.from_numpy(np.asarray(train_target)).long()\n",
        "print(train_target[0])\n",
        "dataset_train = data_utils.TensorDataset(train_input, train_target)\n",
        "train_loader = data_utils.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=False)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jROLV8MQB6d",
        "colab_type": "code",
        "outputId": "ebad2f54-74ae-4b24-bf87-736bb1ea01d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1572
        }
      },
      "source": [
        "# training loop\n",
        "for epoch in range(10):\n",
        "  total_loss = 0\n",
        "  start = time.time()\n",
        "  for i, (context, target) in enumerate(train_loader): #첫번째 context & target set 부터 시작\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      context = context.cuda()\n",
        "      target = target.cuda()\n",
        "\n",
        "      \n",
        "    # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "    # new instance, you need to zero out the gradients from the old instance  \n",
        "    model.zero_grad()\n",
        "\n",
        "    # Step 3. Run the forward pass, getting log probabilities over next words\n",
        "    log_probs = model(context)\n",
        "\n",
        "    # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "    # word wrapped in a tensor)\n",
        "    loss = loss_function(log_probs, target)\n",
        "\n",
        "    # Step 5. Do the backward pass and update the gradient\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(\"loss: %.4f, steps: %dk\" % (loss.item(), ((i+1)/1000)))\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"epochs: %d\" % (epoch+1))\n",
        "  print(\"time eplased: %d seconds\" % (end-start))\n",
        "  print(\"mean loss: %.4f\" % (total_loss / (train_input.shape[0] // BATCH_SIZE)))\n",
        "  \n",
        "  # torch.save: Saves a serialized object to disk. \n",
        "  # Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
        "  torch.save(model.state_dict(), \"cbow.epoch{}.model\".format(epoch))\n",
        "\n",
        "# Here you need to save the model's hidden layer which is V * D word embedding matrix.\n",
        "# Then, use the word embedding matrix to get vectors for word\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 8.5257, steps: 1k\n",
            "loss: 8.1331, steps: 2k\n",
            "loss: 7.6827, steps: 3k\n",
            "loss: 7.5883, steps: 4k\n",
            "loss: 7.5876, steps: 5k\n",
            "loss: 7.6149, steps: 6k\n",
            "epochs: 1\n",
            "time eplased: 107 seconds\n",
            "mean loss: 7.9865\n",
            "loss: 7.4940, steps: 1k\n",
            "loss: 7.2236, steps: 2k\n",
            "loss: 7.1312, steps: 3k\n",
            "loss: 7.2737, steps: 4k\n",
            "loss: 7.1755, steps: 5k\n",
            "loss: 6.7570, steps: 6k\n",
            "epochs: 2\n",
            "time eplased: 110 seconds\n",
            "mean loss: 7.2351\n",
            "loss: 6.9225, steps: 1k\n",
            "loss: 6.9732, steps: 2k\n",
            "loss: 6.9562, steps: 3k\n",
            "loss: 7.0366, steps: 4k\n",
            "loss: 7.1352, steps: 5k\n",
            "loss: 6.7914, steps: 6k\n",
            "epochs: 3\n",
            "time eplased: 109 seconds\n",
            "mean loss: 7.0074\n",
            "loss: 6.9746, steps: 1k\n",
            "loss: 6.9209, steps: 2k\n",
            "loss: 6.8986, steps: 3k\n",
            "loss: 6.8906, steps: 4k\n",
            "loss: 6.9552, steps: 5k\n",
            "loss: 6.9476, steps: 6k\n",
            "epochs: 4\n",
            "time eplased: 109 seconds\n",
            "mean loss: 6.8792\n",
            "loss: 7.0709, steps: 1k\n",
            "loss: 6.8602, steps: 2k\n",
            "loss: 6.6667, steps: 3k\n",
            "loss: 6.5543, steps: 4k\n",
            "loss: 6.7703, steps: 5k\n",
            "loss: 6.7698, steps: 6k\n",
            "epochs: 5\n",
            "time eplased: 110 seconds\n",
            "mean loss: 6.7956\n",
            "loss: 6.6614, steps: 1k\n",
            "loss: 6.8689, steps: 2k\n",
            "loss: 6.7403, steps: 3k\n",
            "loss: 6.4715, steps: 4k\n",
            "loss: 6.5730, steps: 5k\n",
            "loss: 6.8096, steps: 6k\n",
            "epochs: 6\n",
            "time eplased: 109 seconds\n",
            "mean loss: 6.7355\n",
            "loss: 6.7621, steps: 1k\n",
            "loss: 6.7240, steps: 2k\n",
            "loss: 6.5361, steps: 3k\n",
            "loss: 6.6884, steps: 4k\n",
            "loss: 6.7611, steps: 5k\n",
            "loss: 6.6813, steps: 6k\n",
            "epochs: 7\n",
            "time eplased: 109 seconds\n",
            "mean loss: 6.6895\n",
            "loss: 6.7479, steps: 1k\n",
            "loss: 6.9155, steps: 2k\n",
            "loss: 6.6193, steps: 3k\n",
            "loss: 6.6676, steps: 4k\n",
            "loss: 6.6390, steps: 5k\n",
            "loss: 6.6485, steps: 6k\n",
            "epochs: 8\n",
            "time eplased: 110 seconds\n",
            "mean loss: 6.6524\n",
            "loss: 6.5729, steps: 1k\n",
            "loss: 6.5469, steps: 2k\n",
            "loss: 6.6059, steps: 3k\n",
            "loss: 6.4671, steps: 4k\n",
            "loss: 6.7577, steps: 5k\n",
            "loss: 6.6873, steps: 6k\n",
            "epochs: 9\n",
            "time eplased: 109 seconds\n",
            "mean loss: 6.6215\n",
            "loss: 6.7107, steps: 1k\n",
            "loss: 6.4946, steps: 2k\n",
            "loss: 6.5782, steps: 3k\n",
            "loss: 6.5117, steps: 4k\n",
            "loss: 6.4082, steps: 5k\n",
            "loss: 6.4568, steps: 6k\n",
            "epochs: 10\n",
            "time eplased: 110 seconds\n",
            "mean loss: 6.5950\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}