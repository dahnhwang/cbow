{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_cbow_word_analogy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dahnhwang/cbow/blob/master/train_cbow_word_analogy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoFP2MN8nwgr",
        "colab_type": "code",
        "outputId": "bb1ca417-3005-404c-d671-d565f32f4ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "if torch.cuda.is_available():\n",
        "  VOCAB_SIZE = 30000\n",
        "  print('Using Cuda')\n",
        "else:\n",
        "  VOCAB_SIZE = 5000\n",
        "\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "WINDOW_SIZE = 5\n",
        "BATCH_SIZE = 1024\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1ENX-gsfx4",
        "colab_type": "code",
        "outputId": "13e98fd4-a704-4175-e7b9-0d38f9f8bf95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "words = []\n",
        "with open(\"text8.txt\") as f:\n",
        "  for line in f.readlines():\n",
        "    words += line.strip().split(\" \") #라인을 하나씩 읽어들이며 띄어쓰기 단위로 단어들을 분리하여 words array에 집어넣음\n",
        "    \n",
        "print(\"total words in corpus: %d\" % (len(words))) #총 words array 안에 들어간 단어 개수\n",
        "\n",
        "word_cnt = Counter() #컨테이너에 동일한 값의 자료가 몇개인지를 파악하는데 사용하는 객체\n",
        "\n",
        "for w in words:\n",
        "  if w not in word_cnt: #한번도 나타나지 않았던 단어라면 0으로 셋팅\n",
        "    word_cnt[w] = 0\n",
        "  word_cnt[w] += 1 #나타난 단어라면 카운트하기\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total words in corpus: 17005207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBoSRWHwskCB",
        "colab_type": "code",
        "outputId": "9abc64cf-a359-4e7a-cc83-364b94699b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# calculate word coverage of 30k most common words\n",
        "total = 0\n",
        "\n",
        "# print(word_cnt.most_common(VOCAB_SIZE)[0]) ('the', 1061396)\n",
        "\n",
        "for cnt_tup in word_cnt.most_common(VOCAB_SIZE): #word_cnt에서 most common word 3만개 추출\n",
        "  total += cnt_tup[1] \n",
        "print(\"coverage: %.4f \" % (total * 1.0 / len(words))) #전체 단어대비 추출된 단어의 커버리지 퍼센트\n",
        "# 95.94%\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "coverage: 0.9594 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN6ysgvJ4v8e",
        "colab_type": "code",
        "outputId": "ae51dce5-605f-4320-a67d-07faf51bf396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# make vocabulary with most common words\n",
        "word_to_ix = dict()\n",
        "ix_to_word = dict()\n",
        "\n",
        "for i, cnt_tup in enumerate(word_cnt.most_common(VOCAB_SIZE)):\n",
        "  word_to_ix[cnt_tup[0]] = i # 3만개의 각 단어에 대해 고유숫자를 부여\n",
        "  ix_to_word[i] = cnt_tup[0]\n",
        "  last = i\n",
        "  \n",
        "  \n",
        "  \n",
        "print(word_to_ix['banana'])\n",
        "if words[0] not in word_to_ix:\n",
        "  print(words[0])\n",
        "\n",
        "print(ix_to_word[29999])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10967\n",
            "batavian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDBGby4g5JRz",
        "colab_type": "code",
        "outputId": "3a18779c-521c-4db0-bcfa-80f48e8f8636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# add unk token to vocabulary\n",
        "word_to_ix[UNK_TOKEN] = len(word_to_ix) #UNK_TOKEN에 가장 마지막 고유숫자를 부여\n",
        "ix_to_word[len(word_to_ix)] = UNK_TOKEN\n",
        "\n",
        "print(word_to_ix[UNK_TOKEN])\n",
        "\n",
        "# replace rare words in train data with UNK_TOKEN\n",
        "train_words = []\n",
        "count = 0\n",
        "for w in words:\n",
        "  if w not in word_to_ix: #전체단어 중 word_to_ix(고유숫자가 부여된 3만개의 단어)에 없는 것이라면 UNK로 넣기\n",
        "    train_words += [UNK_TOKEN]\n",
        "    count = count + 1\n",
        "  else:\n",
        "    train_words += [w] #있는 것이라면 해당 단어를 그대로 넣기\n",
        "    \n",
        "print(train_words[3953])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n",
            "century\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d_FxQym5Wn8",
        "colab_type": "code",
        "outputId": "988f80b3-48f0-42fd-9506-0cc0ade62dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# make train samples for CBOW\n",
        "train_input = []  #window size 만큼의 주변 context words\n",
        "train_target = [] #context words를 통해 맞춰야 하는 center word\n",
        "span = (WINDOW_SIZE - 1) // 2 #span = 2\n",
        "for i in range(span, len(train_words) - span): # center word 등록하는 숫자\n",
        "  context = []\n",
        "  for j in range(WINDOW_SIZE): # center word 당 주변 context words 등록\n",
        "    if j != span:\n",
        "      context.append(word_to_ix[train_words[i + j - span]]) # center word에서 span뺀만큼에서 i더한 위치\n",
        "  target = word_to_ix[train_words[i]]\n",
        "  train_input.append(context)\n",
        "  train_target.append(target)\n",
        "print(\"data is generated!\") \n",
        "print(train_input[0],train_target[0])\n",
        "\n",
        "for k, v in word_to_ix.items(): #word_to_ix에 아이템을 하나씩 접근해서, key, value를 각각 k, v 저장\n",
        "    if v == train_target[0]:\n",
        "        print(k)\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data is generated!\n",
            "[5233, 3080, 5, 194] 11\n",
            "as\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ0YF4vI5gj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model class\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, window_size):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim) #input layer to projection layer\n",
        "    self.linear1 = nn.Linear(embedding_dim, vocab_size) #projection layer to output layer\n",
        "    pass\n",
        "\n",
        "  def forward(self, inputs): #stacking each layer together\n",
        "    embeds = self.embeddings(inputs)\n",
        "    out = self.linear1(torch.mean(embeds, dim=1)) #임베딩된 결과를 평균함\n",
        "    log_probs = F.log_softmax(out, dim=1) #임베딩된 결과에 활성함수를 취함\n",
        "    return log_probs\n",
        "\n",
        "  def get_word_embedding(self, word):\n",
        "    word = torch.cuda.LongTensor([word_to_ix[word]]) #int를 torch에서 사용하는 자료형으로 변환(?)\n",
        "    return self.embeddings(word).view(1, -1) # ?????\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t5_M7koLNA01",
        "colab": {}
      },
      "source": [
        "# set up to train\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss() #음의 로그 우도 손실 / NLLLoss에 대한 입력 은 로그 확률의 벡터이고 목표 레이블입니다.\n",
        "model = CBOW(len(word_to_ix), EMBEDDING_DIM, WINDOW_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYwTfZsf57TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if gpu is available, then use it\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH_8g9KRP9bD",
        "colab_type": "code",
        "outputId": "266c610b-0455-4078-eca8-07f48029af74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# make data loader for batch training\n",
        "# Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "# into integer indices and wrap them in tensors)\n",
        "train_input = torch.from_numpy(np.asarray(train_input)).long() # Creates a Tensor from a numpy.ndarray\n",
        "train_target = torch.from_numpy(np.asarray(train_target)).long()\n",
        "print(train_target[0])\n",
        "dataset_train = data_utils.TensorDataset(train_input, train_target)\n",
        "train_loader = data_utils.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jROLV8MQB6d",
        "colab_type": "code",
        "outputId": "79cbd8f5-fa50-4fab-d0c4-3e7e1d2fa7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3311
        }
      },
      "source": [
        "# training loop\n",
        "for epoch in range(10):\n",
        "  total_loss = 0\n",
        "  start = time.time()\n",
        "  for i, (context, target) in enumerate(train_loader): #첫번째 context & target set 부터 시작\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      context = context.cuda()\n",
        "      target = target.cuda()\n",
        "\n",
        "      \n",
        "    # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "    # new instance, you need to zero out the gradients from the old instance  \n",
        "    model.zero_grad()\n",
        "\n",
        "    # Step 3. Run the forward pass, getting log probabilities over next words\n",
        "    log_probs = model(context)\n",
        "\n",
        "    # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "    # word wrapped in a tensor)\n",
        "    loss = loss_function(log_probs, target)\n",
        "\n",
        "    # Step 5. Do the backward pass and update the gradient\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(\"loss: %.4f, steps: %dk\" % (loss.item(), ((i+1)/1000)))\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"epochs: %d\" % (epoch+1))\n",
        "  print(\"time eplased: %d seconds\" % (end-start))\n",
        "  print(\"mean loss: %.4f\" % (total_loss / (train_input.shape[0] // BATCH_SIZE)))\n",
        "  \n",
        "  # torch.save: Saves a serialized object to disk. \n",
        "  # Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
        "  torch.save(model.state_dict(), \"cbow.epoch{}.model\".format(epoch))\n",
        "\n",
        "# Here you need to save the model's hidden layer which is V * D word embedding matrix.\n",
        "# Then, use the word embedding matrix to get vectors for word\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 8.5509, steps: 1k\n",
            "loss: 8.0301, steps: 2k\n",
            "loss: 7.7606, steps: 3k\n",
            "loss: 7.6806, steps: 4k\n",
            "loss: 7.4551, steps: 5k\n",
            "loss: 7.4861, steps: 6k\n",
            "loss: 7.4873, steps: 7k\n",
            "loss: 7.3449, steps: 8k\n",
            "loss: 7.1948, steps: 9k\n",
            "loss: 7.2088, steps: 10k\n",
            "loss: 7.1543, steps: 11k\n",
            "loss: 6.9786, steps: 12k\n",
            "loss: 7.2583, steps: 13k\n",
            "loss: 7.1724, steps: 14k\n",
            "loss: 7.1015, steps: 15k\n",
            "loss: 6.9712, steps: 16k\n",
            "epochs: 1\n",
            "time eplased: 495 seconds\n",
            "mean loss: 7.4801\n",
            "loss: 6.7463, steps: 1k\n",
            "loss: 6.7881, steps: 2k\n",
            "loss: 6.9220, steps: 3k\n",
            "loss: 6.8098, steps: 4k\n",
            "loss: 6.8238, steps: 5k\n",
            "loss: 6.8017, steps: 6k\n",
            "loss: 7.1033, steps: 7k\n",
            "loss: 6.7951, steps: 8k\n",
            "loss: 6.8073, steps: 9k\n",
            "loss: 6.8608, steps: 10k\n",
            "loss: 6.7857, steps: 11k\n",
            "loss: 7.0295, steps: 12k\n",
            "loss: 6.7807, steps: 13k\n",
            "loss: 6.7096, steps: 14k\n",
            "loss: 6.7414, steps: 15k\n",
            "loss: 6.6999, steps: 16k\n",
            "epochs: 2\n",
            "time eplased: 481 seconds\n",
            "mean loss: 6.8377\n",
            "loss: 6.8977, steps: 1k\n",
            "loss: 6.6177, steps: 2k\n",
            "loss: 6.7630, steps: 3k\n",
            "loss: 6.4671, steps: 4k\n",
            "loss: 6.6834, steps: 5k\n",
            "loss: 6.7712, steps: 6k\n",
            "loss: 6.7098, steps: 7k\n",
            "loss: 6.7564, steps: 8k\n",
            "loss: 6.4843, steps: 9k\n",
            "loss: 6.7356, steps: 10k\n",
            "loss: 6.6083, steps: 11k\n",
            "loss: 6.5743, steps: 12k\n",
            "loss: 6.6237, steps: 13k\n",
            "loss: 6.6984, steps: 14k\n",
            "loss: 6.6038, steps: 15k\n",
            "loss: 6.7840, steps: 16k\n",
            "epochs: 3\n",
            "time eplased: 499 seconds\n",
            "mean loss: 6.6766\n",
            "loss: 6.6832, steps: 1k\n",
            "loss: 6.5874, steps: 2k\n",
            "loss: 6.6125, steps: 3k\n",
            "loss: 6.6400, steps: 4k\n",
            "loss: 6.7969, steps: 5k\n",
            "loss: 6.5810, steps: 6k\n",
            "loss: 6.5790, steps: 7k\n",
            "loss: 6.5047, steps: 8k\n",
            "loss: 6.5830, steps: 9k\n",
            "loss: 6.8323, steps: 10k\n",
            "loss: 6.4239, steps: 11k\n",
            "loss: 6.4349, steps: 12k\n",
            "loss: 6.4133, steps: 13k\n",
            "loss: 6.5284, steps: 14k\n",
            "loss: 6.8041, steps: 15k\n",
            "loss: 6.7700, steps: 16k\n",
            "epochs: 4\n",
            "time eplased: 504 seconds\n",
            "mean loss: 6.5909\n",
            "loss: 6.6860, steps: 1k\n",
            "loss: 6.5471, steps: 2k\n",
            "loss: 6.5685, steps: 3k\n",
            "loss: 6.4147, steps: 4k\n",
            "loss: 6.7125, steps: 5k\n",
            "loss: 6.6754, steps: 6k\n",
            "loss: 6.5593, steps: 7k\n",
            "loss: 6.4918, steps: 8k\n",
            "loss: 6.6230, steps: 9k\n",
            "loss: 6.4814, steps: 10k\n",
            "loss: 6.6440, steps: 11k\n",
            "loss: 6.6318, steps: 12k\n",
            "loss: 6.5324, steps: 13k\n",
            "loss: 6.6235, steps: 14k\n",
            "loss: 6.4190, steps: 15k\n",
            "loss: 6.5813, steps: 16k\n",
            "epochs: 5\n",
            "time eplased: 503 seconds\n",
            "mean loss: 6.5333\n",
            "loss: 6.6772, steps: 1k\n",
            "loss: 6.6403, steps: 2k\n",
            "loss: 6.6537, steps: 3k\n",
            "loss: 6.5071, steps: 4k\n",
            "loss: 6.2389, steps: 5k\n",
            "loss: 6.3971, steps: 6k\n",
            "loss: 6.6635, steps: 7k\n",
            "loss: 6.3691, steps: 8k\n",
            "loss: 6.4651, steps: 9k\n",
            "loss: 6.3684, steps: 10k\n",
            "loss: 6.6974, steps: 11k\n",
            "loss: 6.6276, steps: 12k\n",
            "loss: 6.4712, steps: 13k\n",
            "loss: 6.4360, steps: 14k\n",
            "loss: 6.5048, steps: 15k\n",
            "loss: 6.3702, steps: 16k\n",
            "epochs: 6\n",
            "time eplased: 501 seconds\n",
            "mean loss: 6.4899\n",
            "loss: 6.5489, steps: 1k\n",
            "loss: 6.1873, steps: 2k\n",
            "loss: 6.5159, steps: 3k\n",
            "loss: 6.3268, steps: 4k\n",
            "loss: 6.4064, steps: 5k\n",
            "loss: 6.3630, steps: 6k\n",
            "loss: 6.6197, steps: 7k\n",
            "loss: 6.3485, steps: 8k\n",
            "loss: 6.4420, steps: 9k\n",
            "loss: 6.4194, steps: 10k\n",
            "loss: 6.3067, steps: 11k\n",
            "loss: 6.4893, steps: 12k\n",
            "loss: 6.5630, steps: 13k\n",
            "loss: 6.5174, steps: 14k\n",
            "loss: 6.3461, steps: 15k\n",
            "loss: 6.4417, steps: 16k\n",
            "epochs: 7\n",
            "time eplased: 501 seconds\n",
            "mean loss: 6.4550\n",
            "loss: 6.5232, steps: 1k\n",
            "loss: 6.5317, steps: 2k\n",
            "loss: 6.5352, steps: 3k\n",
            "loss: 6.3615, steps: 4k\n",
            "loss: 6.5516, steps: 5k\n",
            "loss: 6.4441, steps: 6k\n",
            "loss: 6.4434, steps: 7k\n",
            "loss: 6.4966, steps: 8k\n",
            "loss: 6.3179, steps: 9k\n",
            "loss: 6.4547, steps: 10k\n",
            "loss: 6.5074, steps: 11k\n",
            "loss: 6.4679, steps: 12k\n",
            "loss: 6.6124, steps: 13k\n",
            "loss: 6.3187, steps: 14k\n",
            "loss: 6.5241, steps: 15k\n",
            "loss: 6.4729, steps: 16k\n",
            "epochs: 8\n",
            "time eplased: 502 seconds\n",
            "mean loss: 6.4258\n",
            "loss: 6.5264, steps: 1k\n",
            "loss: 6.6191, steps: 2k\n",
            "loss: 6.4106, steps: 3k\n",
            "loss: 6.4878, steps: 4k\n",
            "loss: 6.4934, steps: 5k\n",
            "loss: 6.3525, steps: 6k\n",
            "loss: 6.4628, steps: 7k\n",
            "loss: 6.3138, steps: 8k\n",
            "loss: 6.2918, steps: 9k\n",
            "loss: 6.4119, steps: 10k\n",
            "loss: 6.3526, steps: 11k\n",
            "loss: 6.3133, steps: 12k\n",
            "loss: 6.5329, steps: 13k\n",
            "loss: 6.2669, steps: 14k\n",
            "loss: 6.4532, steps: 15k\n",
            "loss: 6.1609, steps: 16k\n",
            "epochs: 9\n",
            "time eplased: 482 seconds\n",
            "mean loss: 6.4007\n",
            "loss: 6.4008, steps: 1k\n",
            "loss: 6.2216, steps: 2k\n",
            "loss: 6.2436, steps: 3k\n",
            "loss: 6.4105, steps: 4k\n",
            "loss: 6.2891, steps: 5k\n",
            "loss: 6.3469, steps: 6k\n",
            "loss: 6.4179, steps: 7k\n",
            "loss: 6.4174, steps: 8k\n",
            "loss: 6.4574, steps: 9k\n",
            "loss: 6.1979, steps: 10k\n",
            "loss: 6.2407, steps: 11k\n",
            "loss: 6.4296, steps: 12k\n",
            "loss: 6.4163, steps: 13k\n",
            "loss: 6.3033, steps: 14k\n",
            "loss: 6.4066, steps: 15k\n",
            "loss: 6.5024, steps: 16k\n",
            "epochs: 10\n",
            "time eplased: 482 seconds\n",
            "mean loss: 6.3787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-P1oZwdmOtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_index_of_max(input):\n",
        "    index = 0\n",
        "    for i in range(1, len(input)):\n",
        "        if input[i] > input[index]:\n",
        "            index = i \n",
        "    return index\n",
        "  \n",
        "def get_max_prob_result(input, ix_to_word):\n",
        "    return ix_to_word[get_index_of_max(input)]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7hAPBvRULtE",
        "colab_type": "code",
        "outputId": "178dbf05-49fa-4275-a762-04c7b8d68829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def test_word_analogy(model, ix_to_word, word_to_ix):\n",
        "\n",
        "    # test word analogy\n",
        "    first_line = 1\n",
        "    accuracy = 0\n",
        "    with open(\"questions_words.txt\") as f2:\n",
        "      equal_count = 0\n",
        "      wrong_count = 0\n",
        "      for line in f2.readlines():\n",
        "        analogy_set = line.lower().strip().split(\" \")\n",
        "        if(analogy_set[0]!=':'):\n",
        "          word_vec = []\n",
        "          for i in range(0, len(analogy_set)):\n",
        "            check_word = analogy_set[i]\n",
        "            if check_word not in word_to_ix:\n",
        "                check_word = '<UNK>'\n",
        "            word_vec.append(model.get_word_embedding(check_word)[0].cpu())\n",
        "#             print(analogy_set[i])\n",
        "\n",
        "          resulting_vector = word_vec[0] - word_vec[1] + word_vec[2]\n",
        "          actual_result_vec = word_vec[3]\n",
        "          resulting_word = get_max_prob_result(resulting_vector, ix_to_word)\n",
        "#           print('resulting vector: ', resulting_vector)\n",
        "#           print('actual correct word:', words[3])\n",
        "          if(resulting_word == words[3]):\n",
        "            equal_count = equal_count + 1\n",
        "          else :\n",
        "            wrong_count = wrong_count + 1\n",
        "      print(\"accuracy of word analogy test:\",equal_count / (equal_count+wrong_count))\n",
        "test_word_analogy(model, ix_to_word, word_to_ix)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of word analogy test: 0.007419156774457634\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}